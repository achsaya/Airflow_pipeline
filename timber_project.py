from pyspark.sql import SparkSession
from pyspark.sql.functions import dayofmonth,month,date_format,year
from pyspark.sql.functions import col
spark = SparkSession.builder \
        .appName("timber_capstone") \
        .master("local[1]") \
        .getOrCreate()
df = spark.read.csv("/root/airflow/inputfiles/timberland_stock.csv",header = True)
df.show()
df1 = df.withColumn("day_of_week", date_format("Date", "EEEE"))
df2 = df1.withColumnRenamed("Adj Close","adj_close")
df3 = df2.withColumn("adj_close", col("adj_close").cast("float"))
df4 = df3.withColumn("Volume", col("Volume").cast("float"))
df5 = df4.withColumn("Close", col("Close").cast("float"))
df6 = df5.withColumn("Low", col("Low").cast("float"))
df7 = df6.withColumn("High", col("High").cast("float"))
df7 = df7.withColumn("date", to_date("date", "yyyy-MM-dd"))
df_timber = df7.withColumn("Open", col("Open").cast("float"))
df_timber.printSchema()
df_timber.write.csv("/root/airflow/outputfiles/df_timber",mode="overwrite",header=True)
df_timber.createOrReplaceTempView("timber")
day = spark.sql("select Date,day_of_week,High from timber where High = ( select max(High) from timber having max(High))")
day.show()
day.write.csv("/root/airflow/outputfiles/day",mode="overwrite",header=True)
mean_close = spark.sql("select avg(close) AS Mean_close from timber")
mean_close.show()
mean_close.write.csv("/root/airflow/outputfiles/mean_close",mode="overwrite",header=True)
max_min_volume = spark.sql("select max(Volume),min(Volume) from timber")
max_min_volume.show()
max_min_volume.write.csv("/root/airflow/outputfiles/max_min_volume",mode="overwrite",header=True)
no_of_days = spark.sql("select count(close) from timber where close<60")
no_of_days.show()
no_of_days.write.csv("/root/airflow/outputfiles/no_of_days",mode="overwrite",header=True)
percentage_high = spark.sql("SELECT (count(high) / (select count(*) FROM timber)) * 100 AS Percentage FROM timber where high>80")
percentage_high.show()
percentage_high.write.csv("/root/airflow/outputfiles/percentage_high",mode="overwrite",header=True)
Correlation = spark.sql("SELECT corr(High, Volume) AS PearsonCorrelation FROM timber").collect()[0][0]
print("Pearson Correlation between high and volume: ",Correlation)
max_high_year = spark.sql("select year(date) as Year,max(high) as High from timber group by year(date) having max(high)")
max_high_year.show()
max_high_year.write.csv("/root/airflow/outputfiles/max_high_year",mode="overwrite",header=True)
calendar_month_avg_close = spark.sql("select month(date),year(date),avg(close) from timber group by year(date),month(date) order by year(date),month(date)")
calendar_month_avg_close.show()
from pyspark.sql import SparkSession
from pyspark.sql.functions import dayofmonth,month,date_format,year
from pyspark.sql.functions import col
spark = SparkSession.builder \
        .appName("timber_capstone") \
        .master("local[1]") \
        .getOrCreate()
df = spark.read.csv("/root/airflow/inputfiles/timberland_stock.csv",header = True)
df.show()
df1 = df.withColumn("day_of_week", date_format("Date", "EEEE"))
df2 = df1.withColumnRenamed("Adj Close","adj_close")
df3 = df2.withColumn("adj_close", col("adj_close").cast("float"))
df4 = df3.withColumn("Volume", col("Volume").cast("float"))
df5 = df4.withColumn("Close", col("Close").cast("float"))
df6 = df5.withColumn("Low", col("Low").cast("float"))
df7 = df6.withColumn("High", col("High").cast("float"))
df7 = df7.withColumn("date", to_date("date", "yyyy-MM-dd"))
df_timber = df7.withColumn("Open", col("Open").cast("float"))
df_timber.printSchema()
df_timber.write.csv("/root/airflow/outputfiles/df_timber",mode="overwrite",header=True)
df_timber.createOrReplaceTempView("timber")
day = spark.sql("select Date,day_of_week,High from timber where High = ( select max(High) from timber having max(High))")
day.show()
day.write.csv("/root/airflow/outputfiles/day",mode="overwrite",header=True)
mean_close = spark.sql("select avg(close) AS Mean_close from timber")
mean_close.show()
mean_close.write.csv("/root/airflow/outputfiles/mean_close",mode="overwrite",header=True)
max_min_volume = spark.sql("select max(Volume),min(Volume) from timber")
max_min_volume.show()
max_min_volume.write.csv("/root/airflow/outputfiles/max_min_volume",mode="overwrite",header=True)
no_of_days = spark.sql("select count(close) from timber where close<60")
no_of_days.show()
no_of_days.write.csv("/root/airflow/outputfiles/no_of_days",mode="overwrite",header=True)
percentage_high = spark.sql("SELECT (count(high) / (select count(*) FROM timber)) * 100 AS Percentage FROM timber where high>80")
percentage_high.show()
percentage_high.write.csv("/root/airflow/outputfiles/percentage_high",mode="overwrite",header=True)
Correlation = spark.sql("SELECT corr(High, Volume) AS PearsonCorrelation FROM timber").collect()[0][0]
print("Pearson Correlation between high and volume: ",Correlation)
max_high_year = spark.sql("select year(date) as Year,max(high) as High from timber group by year(date) having max(high)")
max_high_year.show()
max_high_year.write.csv("/root/airflow/outputfiles/max_high_year",mode="overwrite",header=True)
calendar_month_avg_close = spark.sql("select month(date),year(date),avg(close) from timber group by year(date),month(date) order by year(date),month(date)")
calendar_month_avg_close.show()
calendar_month_avg_close.write.csv("/root/airflow/outputfiles/calendar_month_avg_close",mode="overwrite",header=True)
spark.stop()